{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"O objetivo desse projeto é realizar a classificação de operações em cartões de créditos como **fraude** ou **legítimo**.\n\n*Nas versões 1 e 2 eu utilizei Redes Neurais para realizar a predição.* \n\n*Apliquei um ***threshold*** *(limiar) de* ***0.9***, *ou seja, até esse valor, considera-se* ***negativo*** *para fraude e o restante* ***positivo*** *para fraude.*\n\n*O* ***Recall*** *ficou em* ***0.5437*** *e o* ***falso positivo*** *foi cerca de* ***11%*** *de todas as operação que não eram fraude.*\n\nDescobri um desempenho superior no **XGBClassifier**, portanto, escolhi mudar o algorítimo.\n\nCom um **threshold** de **0.863** e **Recall** de **0.5480** (tentei manter os Recalls próximos para avaliar os falsos positivos), o **falso positivo** ficou em torno de **4%**.\n\nIniciaremos com as importações necessárias.","metadata":{}},{"cell_type":"code","source":"# importação das bibliotecas e ferramentas de ML e de métricas\n# manipulação de dados\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import FunctionTransformer, StandardScaler\nfrom category_encoders import TargetEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n# machine learning\nfrom xgboost import XGBClassifier\n\n# métricas\nfrom sklearn.metrics import roc_auc_score # Competition metric\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n\n# joblib\nfrom joblib import dump, load","metadata":{"execution":{"iopub.status.busy":"2023-07-25T17:06:56.430218Z","iopub.execute_input":"2023-07-25T17:06:56.431124Z","iopub.status.idle":"2023-07-25T17:06:56.440305Z","shell.execute_reply.started":"2023-07-25T17:06:56.431079Z","shell.execute_reply":"2023-07-25T17:06:56.439216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Agora, faremos o carregamento dos dados csv no python e depois visualizaremos o **head()**","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/playground-series-s3e4/train.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-25T17:06:56.443023Z","iopub.execute_input":"2023-07-25T17:06:56.443505Z","iopub.status.idle":"2023-07-25T17:06:58.833734Z","shell.execute_reply.started":"2023-07-25T17:06:56.443463Z","shell.execute_reply":"2023-07-25T17:06:58.832502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Após isso, utilizaremos o **describe()** para termos uma visualização da sumarização das colunas.\n\nComo pederemos ver a seguir, os dados são todos **numéricos contínuos**.","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2023-07-25T17:06:58.836463Z","iopub.execute_input":"2023-07-25T17:06:58.836897Z","iopub.status.idle":"2023-07-25T17:06:59.236283Z","shell.execute_reply.started":"2023-07-25T17:06:58.836859Z","shell.execute_reply":"2023-07-25T17:06:59.235047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Aqui, a função **nunique()** servirá para nos dizer quantos dados únicos temos em cada coluna. Dessa forma poderemos saber se há algum possível dado **categórico** convertido para **numérico**.\n\nMas como veremos a seguir, são todos **numéricos** mesmo.","metadata":{}},{"cell_type":"code","source":"df.nunique()","metadata":{"execution":{"iopub.status.busy":"2023-07-25T17:06:59.238308Z","iopub.execute_input":"2023-07-25T17:06:59.238767Z","iopub.status.idle":"2023-07-25T17:06:59.571269Z","shell.execute_reply.started":"2023-07-25T17:06:59.238725Z","shell.execute_reply":"2023-07-25T17:06:59.569681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Feito isso, iremos verificar se há algum **null** com as funções **isnull().sum()**.","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-07-25T17:06:59.574746Z","iopub.execute_input":"2023-07-25T17:06:59.575116Z","iopub.status.idle":"2023-07-25T17:06:59.597075Z","shell.execute_reply.started":"2023-07-25T17:06:59.575084Z","shell.execute_reply":"2023-07-25T17:06:59.595701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nessa etapa o **info()** irá nos mostrar algumas informações sobre as colunas, como por exemplo, o tipo de dado e a quantidade de dados válidos em cada coluna.","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2023-07-25T17:06:59.598629Z","iopub.execute_input":"2023-07-25T17:06:59.598980Z","iopub.status.idle":"2023-07-25T17:06:59.625114Z","shell.execute_reply.started":"2023-07-25T17:06:59.598952Z","shell.execute_reply":"2023-07-25T17:06:59.624256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Agora, analisaremos a quantidade de dados que são fraudes (saída 1) e a quantidade que não são fraudes (saída 0) por tipo de saída (*target*).\n\nIsso é importante, pois como veremos a seguir, há uma quantidade muito grande de *target* 0, ou seja, uma quantidade muito grande de dados que não são fraude e isso acaba por enviezar o modelo de **machine learning**.","metadata":{}},{"cell_type":"code","source":"contagem_classes = df['Class'].value_counts()\nprint(contagem_classes)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T17:06:59.627081Z","iopub.execute_input":"2023-07-25T17:06:59.627731Z","iopub.status.idle":"2023-07-25T17:06:59.636897Z","shell.execute_reply.started":"2023-07-25T17:06:59.627698Z","shell.execute_reply":"2023-07-25T17:06:59.635783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Vamos dividir em dois **DataFrames** os dados de fraude e os dados legítimos.","metadata":{}},{"cell_type":"code","source":"df_fraud = df[df['Class'] == 1]\ndf_legit = df[df['Class'] == 0]\n\nprint(df_fraud['Class'].value_counts())\nprint(df_legit['Class'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2023-07-25T17:06:59.638376Z","iopub.execute_input":"2023-07-25T17:06:59.638729Z","iopub.status.idle":"2023-07-25T17:06:59.674356Z","shell.execute_reply.started":"2023-07-25T17:06:59.638702Z","shell.execute_reply":"2023-07-25T17:06:59.673017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dando, continuidade, agora coletamos uma amostra dos dados não-fraude na mesma quantidade dos dados fraude para igualar a quantidade de cada um, a fim de não enviezar o modelo.\n\nApós isso, concatenaremos as duas tabelas em apenas uma.","metadata":{}},{"cell_type":"code","source":"df_legit = df_legit.sample(n=469, random_state=1)\ndf_reshaped = pd.concat([df_legit,df_fraud])\ndf_reshaped['Class'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-07-25T17:06:59.675676Z","iopub.execute_input":"2023-07-25T17:06:59.676011Z","iopub.status.idle":"2023-07-25T17:06:59.696588Z","shell.execute_reply.started":"2023-07-25T17:06:59.675983Z","shell.execute_reply":"2023-07-25T17:06:59.695284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A partir de agora, iniciaremos nosso pipeline.\n\nA baixo, começaremos criando uma lista com todas as *features* numéricas, que nesse caso serão todas as *features* que temos no nosso DataFrame.","metadata":{}},{"cell_type":"code","source":"# Início do pipeline de treinamento\n# Criação de lista para numerical e Categorical features\n\nnumerical_features = []\n\nfor c in df_reshaped.columns:\n  if c != 'id' and c != 'Class':\n    if df_reshaped[c].dtype != \"dtype('object')\":\n      numerical_features.append(c)\n\nnumerical_features","metadata":{"execution":{"iopub.status.busy":"2023-07-25T17:06:59.698702Z","iopub.execute_input":"2023-07-25T17:06:59.699183Z","iopub.status.idle":"2023-07-25T17:06:59.711475Z","shell.execute_reply.started":"2023-07-25T17:06:59.699141Z","shell.execute_reply":"2023-07-25T17:06:59.710274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Faremos o mesmo para a lista com todas as *features* e com o *target*.","metadata":{}},{"cell_type":"code","source":"# Selecionando features e target\n\nfeatures = list(df_reshaped.columns)\nfor c in ['id', 'Class']:\n  features.remove(c)\n\ntarget = [\n    'Class'\n]\n\nfeatures","metadata":{"execution":{"iopub.status.busy":"2023-07-25T17:06:59.714745Z","iopub.execute_input":"2023-07-25T17:06:59.715504Z","iopub.status.idle":"2023-07-25T17:06:59.727622Z","shell.execute_reply.started":"2023-07-25T17:06:59.715469Z","shell.execute_reply":"2023-07-25T17:06:59.726371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Agora separaremos os dados em **treino** e **teste** para realização do treinamento do modelo.","metadata":{}},{"cell_type":"code","source":"X = df_reshaped.drop(columns= ['id','Class'])\ny = df_reshaped['Class']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T17:06:59.729627Z","iopub.execute_input":"2023-07-25T17:06:59.730037Z","iopub.status.idle":"2023-07-25T17:06:59.743848Z","shell.execute_reply.started":"2023-07-25T17:06:59.730005Z","shell.execute_reply":"2023-07-25T17:06:59.742752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Veremos se os dados estão dividos adequadamente.","metadata":{}},{"cell_type":"code","source":"contClassTrain = y_train.value_counts()\nprint(contClassTrain)\ncontClassTest = y_test.value_counts()\nprint(contClassTest)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T17:06:59.745867Z","iopub.execute_input":"2023-07-25T17:06:59.746304Z","iopub.status.idle":"2023-07-25T17:06:59.755171Z","shell.execute_reply.started":"2023-07-25T17:06:59.746257Z","shell.execute_reply":"2023-07-25T17:06:59.754271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Com a separação realizada, podemos começar a escrever o *pipeline* de preprocessamento.\n\nComo temos apenas dados numéricos, faremos o preprocessamento apenas para ele.\n\nUtilizaremos o método de padronização escalar e substituição dos dados nulos pela mediana do conjunto.\n\nAplicaremos o modelo de classificação ***XGBoost*** e depois o fit (treino) do modelo.","metadata":{}},{"cell_type":"code","source":"# Preprocessamento de colunas numéricas\nnumeric_transformer = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\n# Combinando pré-processadores de colunas numéricas e categóricas\npreprocessor = ColumnTransformer([\n    ('numeric', numeric_transformer, numerical_features)\n])\n\n\n# Criando o pipeline com etapas de pré-processamento e modelo\npipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('lrg', XGBClassifier(learning_rate=0.02, n_estimators=1000, objective='binary:logistic',\n                    nthread=-1, colsample_bytree=0.6,\n                    gamma=1, max_depth=5,min_child_weight=10,subsample=0.8))\n])\n\n# Resetando o índice\nX_train.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T17:06:59.756623Z","iopub.execute_input":"2023-07-25T17:06:59.757484Z","iopub.status.idle":"2023-07-25T17:06:59.767384Z","shell.execute_reply.started":"2023-07-25T17:06:59.757443Z","shell.execute_reply":"2023-07-25T17:06:59.766453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Treinando o pipeline\npipeline.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T17:06:59.768854Z","iopub.execute_input":"2023-07-25T17:06:59.769467Z","iopub.status.idle":"2023-07-25T17:07:01.045014Z","shell.execute_reply.started":"2023-07-25T17:06:59.769436Z","shell.execute_reply":"2023-07-25T17:07:01.043873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Separação da df principal para teste.","metadata":{}},{"cell_type":"code","source":"# separação de dados treino e teste\nX = df[features]\ny = df[target]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T17:07:28.376553Z","iopub.execute_input":"2023-07-25T17:07:28.377791Z","iopub.status.idle":"2023-07-25T17:07:28.479294Z","shell.execute_reply.started":"2023-07-25T17:07:28.377750Z","shell.execute_reply":"2023-07-25T17:07:28.477999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Após o treinamento, faremos a previsão dos dados de teste.","metadata":{}},{"cell_type":"code","source":"y_pred1 = pipeline.predict_proba(X_test)[:,1]\nroc_auc_score(y_test, y_pred1)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T17:07:31.733072Z","iopub.execute_input":"2023-07-25T17:07:31.733921Z","iopub.status.idle":"2023-07-25T17:07:32.121138Z","shell.execute_reply.started":"2023-07-25T17:07:31.733872Z","shell.execute_reply":"2023-07-25T17:07:32.120014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Agora, verificaremos os resultados do teste ajustando o threshold para um valor adequado.","metadata":{}},{"cell_type":"code","source":"# visualização dos resultados obtidos\nthreshold = 0.863\n\ny_pred_binary = list((y_pred1 > threshold).astype(int))\n\ndef printer(y_true, y_pred):\n    print('Accuracy:', accuracy_score(y_true, y_pred))\n    print('Precision:', precision_score(y_true, y_pred))\n    print('Recall:', recall_score(y_true, y_pred))\n    print('F1:', f1_score(y_true, y_pred))\n    print('Confusion Matrix:\\n', confusion_matrix(y_true, y_pred))\n\nprinter(y_test, y_pred_binary)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T17:07:33.700860Z","iopub.execute_input":"2023-07-25T17:07:33.701633Z","iopub.status.idle":"2023-07-25T17:07:33.887435Z","shell.execute_reply.started":"2023-07-25T17:07:33.701586Z","shell.execute_reply":"2023-07-25T17:07:33.886261Z"},"trusted":true},"execution_count":null,"outputs":[]}]}