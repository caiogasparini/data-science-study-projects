{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Este projeto é sobre previsão de rendimento de mirtilo.\n\nO objetivo é prever o *target* ***yield***.\n\n**Atualizações:**\nversão 6: atualizei alguns códigos com objetivo de otimização de tempo de execução e uso de memória.\n\n*Nas versões anteriores eu havia utilizado* ***Linear Regression*** *como método de previsão, tive um* ***MAE: 368.19*** *e* ***R2: 0.8138***.\n\n*Porém olhando o notebook do melhor score desse dataset no* ***Kaggle***, *eu me deparei com o* ***LGBM***, *que é um método que utiliza uma árvore de decisão base e várias outras para reduzir os erros das anteriores.*\n\n*Como esse notebook se trata de um estudo, eu resolvi utilizar esse método para testar como ele funciona. Cheguei a um resultado de* ***MAE: 340.13*** *e* ***R2: 0.8231***.\n\nInicialmente, vamos importar todas as bibliotecas necessárias para a realização desse projeto.","metadata":{}},{"cell_type":"code","source":"# Istalação de bibliotecas\n\n!pip install dtype_diet","metadata":{"execution":{"iopub.status.busy":"2023-08-09T03:15:42.417526Z","iopub.execute_input":"2023-08-09T03:15:42.417964Z","iopub.status.idle":"2023-08-09T03:15:54.951744Z","shell.execute_reply.started":"2023-08-09T03:15:42.417933Z","shell.execute_reply":"2023-08-09T03:15:54.950053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# importação das bibliotecas e ferramentas de ML e de métricas\n# manipulação de dados\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom dtype_diet import report_on_dataframe, optimize_dtypes\n\n# visualização de dados\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import FunctionTransformer, StandardScaler\nfrom category_encoders import TargetEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n# machine learning\nimport lightgbm as lgb\n\n# métricas\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-09T03:16:21.100565Z","iopub.execute_input":"2023-08-09T03:16:21.101039Z","iopub.status.idle":"2023-08-09T03:16:21.108531Z","shell.execute_reply.started":"2023-08-09T03:16:21.101002Z","shell.execute_reply":"2023-08-09T03:16:21.106695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nessa etapa, iremos importar o *dataset* da Kaggle que está em formato CSV.\n\nArmazenaremos ele na variável **'df'** utilizando o modelo de *DataFrame* do **Pandas**.\n\nE por fim, faremos uma visualização do cabeçalho.","metadata":{}},{"cell_type":"code","source":"# importação do dataframe de treino armazenado no kaggle\ndf = pd.read_csv('/kaggle/input/playground-series-s3e14/train.csv')\n\n# visualização de parte dos dados\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-09T03:15:57.250427Z","iopub.execute_input":"2023-08-09T03:15:57.250819Z","iopub.status.idle":"2023-08-09T03:15:57.344987Z","shell.execute_reply.started":"2023-08-09T03:15:57.250785Z","shell.execute_reply":"2023-08-09T03:15:57.343821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No comando abaixo poderemos ver um resumo de informações, como a quantidade de colunas, quantidade de dados *non-null* e o ***Dtype***.","metadata":{}},{"cell_type":"code","source":"# resumo do tipo de dados de cada coluna e a quantidade de dados para cada coluna\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2023-08-09T03:15:57.347714Z","iopub.execute_input":"2023-08-09T03:15:57.348791Z","iopub.status.idle":"2023-08-09T03:15:57.373491Z","shell.execute_reply.started":"2023-08-09T03:15:57.348757Z","shell.execute_reply":"2023-08-09T03:15:57.372463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Com objetivo de otimizar o uso de memória pelo *DataFrame* eu utilizei uma biblioteca chamada ***dtype_diet***.\n\nEssa biblioteca tem uma função chamada ***report_on_dataframe()*** que entrega um *report* em *DataFrame* com os tipos de dados utilizados (normalmente o padrão) e uma sujestão de tipos de dados mais adequados que consomem menos memória.\n\nDentro da própria biblioteca existe uma outra função para fazer a alterção automática desses tipos de dados chamada ***optimize_dtypes()***.\n\nComo podem ver abaixo, tivemos uma redução de aproximadamente **8%** do consumo de memória. Pode não parecer muito, porque nesse caso os tipos de dados padrões já estavam relativamente adequados, mas existem *DataFrames* que eu consegui reduzir cerca de **95%** o consumo de memória. \n\nImagine uma base de dados com **2GB ser reduzida para 100MB**... Isso te coloca em outro patamar, meu amigo!","metadata":{}},{"cell_type":"code","source":"# Variável com o uso de memória do DF original já convertido para MB\noriginal_memory = df.memory_usage(deep=True).sum()/1024/1024\n\n# Criando DF com os tipos de dados e possiveis otmizações\ndf_report = report_on_dataframe(df)\n\n# Substituindo o DF antigo pelo novo DF otimizado\ndf = optimize_dtypes(df, df_report)\n\n# Variável com o uso de memória do novo DF já convertido para MB\nnew_memory = df.memory_usage(deep=True).sum()/1024/1024\n\n# Visualização do consumo e da redução do consumo\nprint(f'Original df memory: {original_memory} MB')\nprint(f'New df memory: {new_memory} MB')\nprint(f'Reduction: {(1 - (new_memory / original_memory)) * 100:.02f}%')","metadata":{"execution":{"iopub.status.busy":"2023-08-09T03:15:57.374953Z","iopub.execute_input":"2023-08-09T03:15:57.375363Z","iopub.status.idle":"2023-08-09T03:15:57.498903Z","shell.execute_reply.started":"2023-08-09T03:15:57.375326Z","shell.execute_reply":"2023-08-09T03:15:57.497757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Analisando o cabeçalho já conseguimos identificar que todos os dados são numéricos, sendo quase todos contínuos. Apenas a *feature* ***'RainingDays'*** que parece ser discreto.\n\nPara termos mais informações, utilizaremos a sumarização a seguir:","metadata":{}},{"cell_type":"code","source":"# sumarização dos dados para encontrar outliers ou anomalias\ndf.describe()","metadata":{"execution":{"iopub.status.busy":"2023-08-09T03:15:57.500301Z","iopub.execute_input":"2023-08-09T03:15:57.500728Z","iopub.status.idle":"2023-08-09T03:15:57.576267Z","shell.execute_reply.started":"2023-08-09T03:15:57.500690Z","shell.execute_reply":"2023-08-09T03:15:57.575205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Aparentemente os dados parecem normais, com excessão da *feature* ***'honeybee'***, onde podemos ver que o valor máximo dela é muito distante dos demais valores.\n\nTrataremos isso mais adiante na construção dos *pipelines*.\n\nA seguir, faremos uma contagem de quantos *outliers* temos na feature ***honeybee***.","metadata":{}},{"cell_type":"code","source":"# contagem de quantos outliers existem na feature 'honeybee'\nmediana = df['honeybee'].median()\ndpad = df['honeybee'].std()\nfiltro = mediana + (2 * dpad)\nprint(f'{df[\"honeybee\"][df[\"honeybee\"] > filtro].count()} Outliers')","metadata":{"execution":{"iopub.status.busy":"2023-08-09T03:18:40.788991Z","iopub.execute_input":"2023-08-09T03:18:40.789378Z","iopub.status.idle":"2023-08-09T03:18:40.799506Z","shell.execute_reply.started":"2023-08-09T03:18:40.789350Z","shell.execute_reply":"2023-08-09T03:18:40.798322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Agora, para termos uma ideia de quais *features* tem mais relevância no *target*, faremos uma *list comprehension* (até a versão 5 eu havia utilizado laço de repetição) para ver a correlação de cada *feature* com o nosso *target*.\n\nA vantagem da *list comprehension* é que ela executa muito mais rápido em diversos casos.\n\nNa versão 5 eu criei um *for in* com *if* para visualizar a correlação de cada variável independente em relação a variável dependente e tive um tempo de execução de **12.6 ms**, já com a *list comprehension* consegui o mesmo resultado com apenas **6.11 ms**.\n\nLembrando novamente, esses tempos são pequenos, pois essa base é pequena. Mas isso é uma ótima pratica quando se utiliza de bases gigantescas.","metadata":{}},{"cell_type":"code","source":"#%%timeit # utilizado para ver o tempo gasto para executar o bloco\n# Visualização da correlação de cada variável independente em relação à variável dependente\n\nexclusion = ['id','yield']\ncorr_rounded = pd.DataFrame()\ncorr_rounded[0] = pd.DataFrame([c for c in df.columns if c not in exclusion])\ncorr_rounded[1] = round(pd.DataFrame([df[c].corr(df['yield']) for c in df.columns if c not in exclusion]), 3)\ncorr_rounded","metadata":{"execution":{"iopub.status.busy":"2023-08-09T03:15:57.589064Z","iopub.execute_input":"2023-08-09T03:15:57.589373Z","iopub.status.idle":"2023-08-09T03:15:57.619025Z","shell.execute_reply.started":"2023-08-09T03:15:57.589347Z","shell.execute_reply":"2023-08-09T03:15:57.618103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Como vimos acima, as três últimas *features* tem uma correlação forte com o *target*.\n\nAbaixo, iremos plotar essas três *features* em um gráfico *scatter* para vermos a distribuição dos dados nele.","metadata":{}},{"cell_type":"code","source":"# plotagem das três maiores correlações com o yield\nfig, axs = plt.subplots(nrows=2, ncols=3,figsize=(20, 10))\n\n# Criando uma coluna no DataFrame para indicar a cor dos pontos com base na condição\ncolor = np.where(df['RainingDays'] > df['RainingDays'].mean(),'maior','menor/igual')\naxs = axs.flatten()\n\nfor i in range(3):\n    sns.scatterplot(x=df.iloc[:, i + 14], y=df['yield'],hue=color, ax=axs[i], palette=['blue','orange'])\n    axs[i].set_xlabel(df.columns[i + 14])\n    axs[i].set_ylabel('Yield')\n\nfor i in range(3,6):\n    sns.scatterplot(x=df.iloc[:, i + 8], y=df['yield'],hue=color, ax=axs[i], palette=['blue','orange'])\n    axs[i].set_xlabel(df.columns[i + 8])\n    axs[i].set_ylabel('Yield')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-09T03:15:57.620631Z","iopub.execute_input":"2023-08-09T03:15:57.621041Z","iopub.status.idle":"2023-08-09T03:16:04.180134Z","shell.execute_reply.started":"2023-08-09T03:15:57.621002Z","shell.execute_reply":"2023-08-09T03:16:04.179078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Podemos ver nos gráficos, que existe uma tendência de \"desfunilamento\". Quero dizer que, quanto maior o valor da *feature*, maior é o valor do *target* porém com uma amplitude maior também.\n\nJá da para termos uma ideia de que nosso modelo provavelmente irá errar mais na predição dos *targets* maiores.\n\nA partir de agora, iniciaremos nosso *pipeline*.\n\nNa versão 5, eu usei o *for in* com dois *if* e comecei pelas *features* numéricas, dessa vez faremos de forma mais eficiente.\n\nAbaixo, começaremos criando uma *list comprehension* com todas as *features* e declaramos qual o *target*.\n\n*Detalhe: Como temos apenas features numéricas, eu poderia ter feito de uma forma mais simples, apenas removendo as colunas 'id' e 'yield', porém preferi manter esse esquema, pois essa é uma forma automatizada  e de boa prática de criar a lista. Nesse caso não foi necessariamente mais eficiente, mas em outros casos pode ser*.","metadata":{}},{"cell_type":"code","source":"#%%timeit # utilizado para ver o tempo gasto para executar o bloco\n# Selecionando features e target\n\nfeatures = [c for c in df.columns if c not in ['id', 'yield']]\n\ntarget = [\n    'yield'\n]\n\nfeatures","metadata":{"execution":{"iopub.status.busy":"2023-08-09T03:16:04.182633Z","iopub.execute_input":"2023-08-09T03:16:04.183689Z","iopub.status.idle":"2023-08-09T03:16:04.192266Z","shell.execute_reply.started":"2023-08-09T03:16:04.183639Z","shell.execute_reply":"2023-08-09T03:16:04.191062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%timeit # utilizado para ver o tempo gasto para executar o bloco\n# Início do pipeline de treinamento\n# Criação de lista para numerical e Categorical features\n\nnumerical_features = [c for c in df.columns if c != 'id' and c != 'yield' if df[c].dtype != 'object']\ncategorical_features = list(set(features) - set(numerical_features))\n\nprint(\"Numerical Features:\", numerical_features)\nprint(\"Categorical Features:\", categorical_features)","metadata":{"execution":{"iopub.status.busy":"2023-08-09T03:16:04.194225Z","iopub.execute_input":"2023-08-09T03:16:04.194664Z","iopub.status.idle":"2023-08-09T03:16:04.210537Z","shell.execute_reply.started":"2023-08-09T03:16:04.194602Z","shell.execute_reply":"2023-08-09T03:16:04.209556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Agora faremos a separação dos dados de treino e teste.","metadata":{}},{"cell_type":"code","source":"# separação de dados treino e teste\nX = df[features]\ny = df[target]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-08-09T03:16:04.211673Z","iopub.execute_input":"2023-08-09T03:16:04.212372Z","iopub.status.idle":"2023-08-09T03:16:04.231990Z","shell.execute_reply.started":"2023-08-09T03:16:04.212339Z","shell.execute_reply":"2023-08-09T03:16:04.230977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Com a separação realizada, podemos começar a escrever o *pipeline* de preprocessamento. \n\nComo temos apenas dados numéricos, faremos o preprocessamento apenas para ele.\n\nSuavizaremos os *outliers* utilizando logarítimo.\n\nUtilizaremos o método de padronização escalar e substituição dos dados nulos pela mediana do conjunto.\n\nAplicaremos o modelo de **regressão LGBM** e depois o *fit* (treino) do modelo.","metadata":{}},{"cell_type":"code","source":"# Preprocessamento de outliers de colunas numéricas\noutliers = Pipeline([\n    ('transformer', FunctionTransformer(np.log1p))\n])\n\n# Preprocessamento de colunas numéricas\nnumeric_transformer = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\n# Combinando pré-processadores de colunas numéricas e categóricas\npreprocessor = ColumnTransformer([\n    ('outliers', outliers, numerical_features),\n    ('numeric', numeric_transformer, numerical_features)\n])\n\n# Criando o pipeline com etapas de pré-processamento e modelo\npipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('model', lgb.LGBMRegressor(\n        objective= \"regression_l1\",\n        metric= \"mae\"\n))\n])\n\n# Resetando o índice\nX_train.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)\n\n# Treinando o pipeline\npipeline.fit(X_train, y_train.values.ravel())","metadata":{"execution":{"iopub.status.busy":"2023-08-09T03:39:16.894079Z","iopub.execute_input":"2023-08-09T03:39:16.894451Z","iopub.status.idle":"2023-08-09T03:39:17.793005Z","shell.execute_reply.started":"2023-08-09T03:39:16.894424Z","shell.execute_reply":"2023-08-09T03:39:17.791704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Após o treinamento, faremos a previsão dos dados de teste.","metadata":{}},{"cell_type":"code","source":"# Fazendo predição com o modelo treinado no pipeline\ny_pred = pipeline.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-08-09T03:39:24.493403Z","iopub.execute_input":"2023-08-09T03:39:24.493838Z","iopub.status.idle":"2023-08-09T03:39:24.513494Z","shell.execute_reply.started":"2023-08-09T03:39:24.493783Z","shell.execute_reply":"2023-08-09T03:39:24.512546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"E por fim, a visualização dos resultados.","metadata":{}},{"cell_type":"code","source":"# visualização dos resultados obtidos\nprint(f'R2: {r2_score(y_test, y_pred)} --> Quantos % a predição representa o resultado real')\nprint(f'MAE: {mean_absolute_error(y_test, y_pred)} --> Erro médio absoluto')\nprint(f'MSE: {mean_squared_error(y_test, y_pred)} --> Erro médio quadrático')","metadata":{"execution":{"iopub.status.busy":"2023-08-09T03:39:27.244316Z","iopub.execute_input":"2023-08-09T03:39:27.244733Z","iopub.status.idle":"2023-08-09T03:39:27.256311Z","shell.execute_reply.started":"2023-08-09T03:39:27.244700Z","shell.execute_reply":"2023-08-09T03:39:27.255267Z"},"trusted":true},"execution_count":null,"outputs":[]}]}